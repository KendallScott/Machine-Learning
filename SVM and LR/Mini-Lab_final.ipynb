{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KendallScott/Machine-Learning/blob/main/SVM%20and%20LR/Mini-Lab_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contributors\n",
        "* Tadd Backus\n",
        "* Kendall Scott\n",
        "* Austin Webb\n",
        "* Milan Patel\n"
      ],
      "metadata": {
        "id": "e6TGIIukk6c-"
      },
      "id": "e6TGIIukk6c-"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "HTML('''<script>\n",
        "code_show_err=false; \n",
        "function code_toggle_err() {\n",
        " if (code_show_err){\n",
        " $('div.output_stderr').hide();\n",
        " } else {\n",
        " $('div.output_stderr').show();\n",
        " }\n",
        " code_show_err = !code_show_err\n",
        "} \n",
        "$( document ).ready(code_toggle_err);\n",
        "</script>\n",
        "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
      ],
      "metadata": {
        "id": "GUBk6wGhk3v8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "194751ee-eee2-4cb2-ac01-7f4f5781de7a"
      },
      "id": "GUBk6wGhk3v8",
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script>\n",
              "code_show_err=false; \n",
              "function code_toggle_err() {\n",
              " if (code_show_err){\n",
              " $('div.output_stderr').hide();\n",
              " } else {\n",
              " $('div.output_stderr').show();\n",
              " }\n",
              " code_show_err = !code_show_err\n",
              "} \n",
              "$( document ).ready(code_toggle_err);\n",
              "</script>\n",
              "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "dcfc8062",
      "metadata": {
        "id": "dcfc8062"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics as mt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "pd.set_option('display.max_columns',None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "72389e27",
      "metadata": {
        "id": "72389e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "e68dbe3b-4a6a-4b83-9dbb-ee784cf7608d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  \\\n",
              "0       2596      51      3                               258   \n",
              "1       2590      56      2                               212   \n",
              "2       2804     139      9                               268   \n",
              "3       2785     155     18                               242   \n",
              "4       2595      45      2                               153   \n",
              "\n",
              "   Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  \\\n",
              "0                               0                              510   \n",
              "1                              -6                              390   \n",
              "2                              65                             3180   \n",
              "3                             118                             3090   \n",
              "4                              -1                              391   \n",
              "\n",
              "   Hillshade_9am  Hillshade_Noon  Hillshade_3pm  \\\n",
              "0            221             232            148   \n",
              "1            220             235            151   \n",
              "2            234             238            135   \n",
              "3            238             238            122   \n",
              "4            220             234            150   \n",
              "\n",
              "   Horizontal_Distance_To_Fire_Points  Rawah  Neota  Comanche Peak  \\\n",
              "0                                6279      1      0              0   \n",
              "1                                6225      1      0              0   \n",
              "2                                6121      1      0              0   \n",
              "3                                6211      1      0              0   \n",
              "4                                6172      1      0              0   \n",
              "\n",
              "   Cache la Poudre  Soil_Type1  Soil_Type2  Soil_Type3  Soil_Type4  \\\n",
              "0                0           0           0           0           0   \n",
              "1                0           0           0           0           0   \n",
              "2                0           0           0           0           0   \n",
              "3                0           0           0           0           0   \n",
              "4                0           0           0           0           0   \n",
              "\n",
              "   Soil_Type5  Soil_Type6  Soil_Type7  Soil_Type8  Soil_Type9  Soil_Type10  \\\n",
              "0           0           0           0           0           0            0   \n",
              "1           0           0           0           0           0            0   \n",
              "2           0           0           0           0           0            0   \n",
              "3           0           0           0           0           0            0   \n",
              "4           0           0           0           0           0            0   \n",
              "\n",
              "   Soil_Type11  Soil_Type12  Soil_Type13  Soil_Type14  Soil_Type15  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            1            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Soil_Type16  Soil_Type17  Soil_Type18  Soil_Type19  Soil_Type20  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Soil_Type21  Soil_Type22  Soil_Type23  Soil_Type24  Soil_Type25  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Soil_Type26  Soil_Type27  Soil_Type28  Soil_Type29  Soil_Type30  \\\n",
              "0            0            0            0            1            0   \n",
              "1            0            0            0            1            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            1   \n",
              "4            0            0            0            1            0   \n",
              "\n",
              "   Soil_Type31  Soil_Type32  Soil_Type33  Soil_Type34  Soil_Type35  \\\n",
              "0            0            0            0            0            0   \n",
              "1            0            0            0            0            0   \n",
              "2            0            0            0            0            0   \n",
              "3            0            0            0            0            0   \n",
              "4            0            0            0            0            0   \n",
              "\n",
              "   Soil_Type36  Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type  \n",
              "0            0            0            0            0            0           5  \n",
              "1            0            0            0            0            0           5  \n",
              "2            0            0            0            0            0           2  \n",
              "3            0            0            0            0            0           2  \n",
              "4            0            0            0            0            0           5  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dcb7c3c-03ac-412c-b4d7-24af097406d4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Elevation</th>\n",
              "      <th>Aspect</th>\n",
              "      <th>Slope</th>\n",
              "      <th>Horizontal_Distance_To_Hydrology</th>\n",
              "      <th>Vertical_Distance_To_Hydrology</th>\n",
              "      <th>Horizontal_Distance_To_Roadways</th>\n",
              "      <th>Hillshade_9am</th>\n",
              "      <th>Hillshade_Noon</th>\n",
              "      <th>Hillshade_3pm</th>\n",
              "      <th>Horizontal_Distance_To_Fire_Points</th>\n",
              "      <th>Rawah</th>\n",
              "      <th>Neota</th>\n",
              "      <th>Comanche Peak</th>\n",
              "      <th>Cache la Poudre</th>\n",
              "      <th>Soil_Type1</th>\n",
              "      <th>Soil_Type2</th>\n",
              "      <th>Soil_Type3</th>\n",
              "      <th>Soil_Type4</th>\n",
              "      <th>Soil_Type5</th>\n",
              "      <th>Soil_Type6</th>\n",
              "      <th>Soil_Type7</th>\n",
              "      <th>Soil_Type8</th>\n",
              "      <th>Soil_Type9</th>\n",
              "      <th>Soil_Type10</th>\n",
              "      <th>Soil_Type11</th>\n",
              "      <th>Soil_Type12</th>\n",
              "      <th>Soil_Type13</th>\n",
              "      <th>Soil_Type14</th>\n",
              "      <th>Soil_Type15</th>\n",
              "      <th>Soil_Type16</th>\n",
              "      <th>Soil_Type17</th>\n",
              "      <th>Soil_Type18</th>\n",
              "      <th>Soil_Type19</th>\n",
              "      <th>Soil_Type20</th>\n",
              "      <th>Soil_Type21</th>\n",
              "      <th>Soil_Type22</th>\n",
              "      <th>Soil_Type23</th>\n",
              "      <th>Soil_Type24</th>\n",
              "      <th>Soil_Type25</th>\n",
              "      <th>Soil_Type26</th>\n",
              "      <th>Soil_Type27</th>\n",
              "      <th>Soil_Type28</th>\n",
              "      <th>Soil_Type29</th>\n",
              "      <th>Soil_Type30</th>\n",
              "      <th>Soil_Type31</th>\n",
              "      <th>Soil_Type32</th>\n",
              "      <th>Soil_Type33</th>\n",
              "      <th>Soil_Type34</th>\n",
              "      <th>Soil_Type35</th>\n",
              "      <th>Soil_Type36</th>\n",
              "      <th>Soil_Type37</th>\n",
              "      <th>Soil_Type38</th>\n",
              "      <th>Soil_Type39</th>\n",
              "      <th>Soil_Type40</th>\n",
              "      <th>Cover_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596</td>\n",
              "      <td>51</td>\n",
              "      <td>3</td>\n",
              "      <td>258</td>\n",
              "      <td>0</td>\n",
              "      <td>510</td>\n",
              "      <td>221</td>\n",
              "      <td>232</td>\n",
              "      <td>148</td>\n",
              "      <td>6279</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590</td>\n",
              "      <td>56</td>\n",
              "      <td>2</td>\n",
              "      <td>212</td>\n",
              "      <td>-6</td>\n",
              "      <td>390</td>\n",
              "      <td>220</td>\n",
              "      <td>235</td>\n",
              "      <td>151</td>\n",
              "      <td>6225</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804</td>\n",
              "      <td>139</td>\n",
              "      <td>9</td>\n",
              "      <td>268</td>\n",
              "      <td>65</td>\n",
              "      <td>3180</td>\n",
              "      <td>234</td>\n",
              "      <td>238</td>\n",
              "      <td>135</td>\n",
              "      <td>6121</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785</td>\n",
              "      <td>155</td>\n",
              "      <td>18</td>\n",
              "      <td>242</td>\n",
              "      <td>118</td>\n",
              "      <td>3090</td>\n",
              "      <td>238</td>\n",
              "      <td>238</td>\n",
              "      <td>122</td>\n",
              "      <td>6211</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>153</td>\n",
              "      <td>-1</td>\n",
              "      <td>391</td>\n",
              "      <td>220</td>\n",
              "      <td>234</td>\n",
              "      <td>150</td>\n",
              "      <td>6172</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dcb7c3c-03ac-412c-b4d7-24af097406d4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5dcb7c3c-03ac-412c-b4d7-24af097406d4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5dcb7c3c-03ac-412c-b4d7-24af097406d4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "forest_cover_type = pd.read_csv('covtype.csv')\n",
        "forest_cover_type.head()\n",
        "# Creating a new dataset that only contains the most common cover types\n",
        "#updated to all\n",
        "# Renaming wilderness areas\n",
        "forestDF = forest_cover_type\n",
        "forestDF.rename(columns={'Wilderness_Area1':'Rawah','Wilderness_Area2':'Neota','Wilderness_Area3':'Comanche Peak','Wilderness_Area4':'Cache la Poudre'},inplace=True)\n",
        "forestDF.head()\n",
        "\n",
        "\n",
        "# Coding Lodgepole Pine as 1 and Spruce/Fir as 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#New dataframe that only contains two most common cover types\n",
        "# 1 -> Spruce/Fir\n",
        "# 2 -> Lodgepole Pine\n",
        "\n",
        "forestDF = forest_cover_type[(forest_cover_type['Cover_Type']==1) | (forest_cover_type['Cover_Type']==2)]"
      ],
      "metadata": {
        "id": "VBSnqPvc61AF"
      },
      "id": "VBSnqPvc61AF",
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forestDF = forestDF.astype({'Cover_Type':'string'})\n",
        "forestDF['Cover_Type'] = forestDF['Cover_Type'].str.replace('1','Spruce/Fir')\n",
        "forestDF['Cover_Type'] = forestDF['Cover_Type'].str.replace('2','Lodgepole Pine')\n",
        "forestDF['Cover_Type'].describe().transpose()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngJX22Uv72LL",
        "outputId": "79b67da7-0125-41f3-c4b9-d03f15543db0"
      },
      "id": "ngJX22Uv72LL",
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count             495141\n",
              "unique                 2\n",
              "top       Lodgepole Pine\n",
              "freq              283301\n",
              "Name: Cover_Type, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lodgepole Pine = 1 | Spruce/Fir = 0\n",
        "forestDF['treeType'] = forestDF.Cover_Type == 'Lodgepole Pine'\n",
        "forestDF.treeType = forestDF.treeType.astype(np.int64)\n",
        "if 'Cover_Type' in forestDF:\n",
        "    del forestDF['Cover_Type']\n"
      ],
      "metadata": {
        "id": "3sB11uGjuupI"
      },
      "id": "3sB11uGjuupI",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forestFullDF = forestDF.copy()"
      ],
      "metadata": {
        "id": "D0a1Nigf7B4M"
      },
      "id": "D0a1Nigf7B4M",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Models\n",
        "For our models, we decided to predict Spruce/Fir.\n",
        "\n",
        "After comparing the results, we will make a recommendation for which model to implement in order to predict Cover Type."
      ],
      "metadata": {
        "id": "8zN4Z9ULBc-c"
      },
      "id": "8zN4Z9ULBc-c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Create Models 50\n",
        "\n",
        "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. \n",
        "Assess how well each model performs (use 80/20 training/testing split for your data). \n",
        "Adjust parameters of the models to make them more accurate. \n",
        "If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. \n",
        "That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
      ],
      "metadata": {
        "id": "gg2dVWM5hsJp"
      },
      "id": "gg2dVWM5hsJp"
    },
    {
      "cell_type": "markdown",
      "id": "7bd81bde",
      "metadata": {
        "id": "7bd81bde"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "forestFullDF = forestDF.copy()"
      ],
      "metadata": {
        "id": "IFxgRtVj8iKM"
      },
      "id": "IFxgRtVj8iKM",
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "11bb0b87",
      "metadata": {
        "id": "11bb0b87"
      },
      "outputs": [],
      "source": [
        "if 'treeType' in forestDF:\n",
        "    Y = forestDF['treeType'].values\n",
        "    del forestDF['treeType']\n",
        "    X = forestDF.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "68d276d2",
      "metadata": {
        "id": "68d276d2"
      },
      "outputs": [],
      "source": [
        "#Creating cross validation object\n",
        "num_cv_iter = 10\n",
        "num_instances = len(Y)\n",
        "cv_Log = ShuffleSplit(n_splits=num_cv_iter,test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "eadf2078",
      "metadata": {
        "id": "eadf2078"
      },
      "outputs": [],
      "source": [
        "# Creating logistic regression object\n",
        "lr_clf = LogisticRegression(penalty = 'l2',\n",
        "                            C = 1.0,\n",
        "                            class_weight = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aab106f",
      "metadata": {
        "id": "9aab106f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd55fbe1-d655-4670-c177-36b389f88e10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****Iteration 0 ****\n",
            "Accuracy 0.7497601712629633\n",
            "Confusion Matrix \n",
            " [[28751 13415]\n",
            " [11366 45497]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****Iteration 1 ****\n",
            "Accuracy 0.743368104292682\n",
            "Confusion Matrix \n",
            " [[29095 13316]\n",
            " [12098 44520]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****Iteration 2 ****\n",
            "Accuracy 0.7453170283452322\n",
            "Confusion Matrix \n",
            " [[28801 13558]\n",
            " [11663 45007]]\n"
          ]
        }
      ],
      "source": [
        "# Running logistic regression on 10 random test/train splits\n",
        "iter_num = 0\n",
        "\n",
        "for iter_num, (train_indices,test_indices) in enumerate(cv_Log.split(X,Y)):\n",
        "    lr_clf.fit(X[train_indices],Y[train_indices])\n",
        "    y_hat = lr_clf.predict(X[test_indices])\n",
        "    \n",
        "    print('****Iteration',iter_num,'****')\n",
        "    print('Accuracy',mt.accuracy_score(Y[test_indices],y_hat))\n",
        "    print('Confusion Matrix \\n',mt.confusion_matrix(Y[test_indices],y_hat))\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the logistic regression model was run ten times, the accuracy score appeared to remain near 61%. This shows us that the different train/test splits seem to be fair in their distributions. We do anticipate a change in accuracy (an increase) as we attempt to adjust some parameters and implement other measures. "
      ],
      "metadata": {
        "id": "hr0hJhUow7By"
      },
      "id": "hr0hJhUow7By"
    },
    {
      "cell_type": "code",
      "source": [
        "logWeights = pd.Series(lr_clf.coef_[0], index = forestDF.columns)\n",
        "logWeights = pd.DataFrame({'Var':logWeights.index, 'Weight':logWeights.values})\n",
        "logWeights = logWeights.sort_values('Weight')\n",
        "plt.figure(figsize=(8, 10))\n",
        "sns.barplot(x='Weight', y = 'Var', data = logWeights)"
      ],
      "metadata": {
        "id": "QVQwXW_v6KSA"
      },
      "id": "QVQwXW_v6KSA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to leverage Random Forest to eliminate some of the variables included in the model, to see if there is an increase in performance in a simpler model."
      ],
      "metadata": {
        "id": "ahvN4uSBqoW0"
      },
      "id": "ahvN4uSBqoW0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance Graph\n",
        "# Takes about five minutes\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(n_estimators = 20)\n",
        "rf.fit(X, Y)"
      ],
      "metadata": {
        "id": "QU5toe0grs3B"
      },
      "id": "QU5toe0grs3B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sort = rf.feature_importances_.argsort()\n",
        "plt.barh(forestDF.columns.values[sort], rf.feature_importances_[sort])\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.axis([0, 0.35, 34, 52])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l409CPrduYOy"
      },
      "id": "l409CPrduYOy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forestDF.columns"
      ],
      "metadata": {
        "id": "aBhlTwtCzqKm"
      },
      "id": "aBhlTwtCzqKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the importance output, we decided to build a simplified model on:\n",
        "\n",
        "*   Elevation\n",
        "*   Horizontal Distance to Fire Points\n",
        "*   Soil Type: 32, 4, 2, 23, 29, 33, 31, 22\n",
        "*   Horizontal Distance to Roadways\n",
        "*   Wilderness Area: Comanche Peak, Rawah\n",
        "*   Horizontal Distance To Hydrology\n",
        "*   Hillshade noon\n",
        "*   Aspect\n",
        "*   Slope\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kq5Xrc7wrBHe"
      },
      "id": "Kq5Xrc7wrBHe"
    },
    {
      "cell_type": "code",
      "source": [
        "forestDF_new = pd.DataFrame().assign(\n",
        "                treeType=Y, \n",
        "                Elevation=forestDF['Elevation'], \n",
        "                Horizontal_Distance_To_Fire_Points=forestDF['Horizontal_Distance_To_Fire_Points'],\n",
        "                Soil_Type32=forestDF['Soil_Type32'],\n",
        "                Soil_Type4=forestDF['Soil_Type4'],\n",
        "                Soil_Type2=forestDF['Soil_Type2'],\n",
        "                Soil_Type23=forestDF['Soil_Type23'],\n",
        "                Soil_Type29=forestDF['Soil_Type29'],\n",
        "                Soil_Type33=forestDF['Soil_Type33'],\n",
        "                Soil_Type31=forestDF['Soil_Type31'],\n",
        "                Soil_Type22=forestDF['Soil_Type22'],\n",
        "                Horizontal_Distance_To_Roadways=forestDF['Horizontal_Distance_To_Roadways'],\n",
        "                Comanche_Peak=forestDF['Comanche Peak'], \n",
        "                Rawah=forestDF['Rawah'], \n",
        "                Horizontal_Distance_To_Hydrology=forestDF['Horizontal_Distance_To_Hydrology'],\n",
        "                Hillshade_Noon=forestDF['Hillshade_Noon'],\n",
        "                Aspect=forestDF['Aspect'],\n",
        "                Slope=forestDF['Slope']\n",
        "                                                      )\n"
      ],
      "metadata": {
        "id": "2nu7kYPa0WD9"
      },
      "id": "2nu7kYPa0WD9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign specific features to two different variables\n",
        "if 'treeType' in forestDF:\n",
        "    Y = forestDF_new['treeType'].values\n",
        "    del forestDF_new['treeType']\n",
        "    X = forestDF_new.values"
      ],
      "metadata": {
        "id": "ImGv5F8Tw0uU"
      },
      "id": "ImGv5F8Tw0uU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-creating logistic regression object\n",
        "lr_clf_new = LogisticRegression(penalty = 'l2', C = 1.0, class_weight = None)"
      ],
      "metadata": {
        "id": "UX1Oh210w-Oj"
      },
      "id": "UX1Oh210w-Oj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running logistic regression on 10 random test/train splits with simplified model\n",
        "iter_num = 0\n",
        "\n",
        "for iter_num, (train_indices,test_indices) in enumerate(cv_object.split(X,Y)):\n",
        "    lr_clf_new.fit(X[train_indices],Y[train_indices])\n",
        "    y_hat = lr_clf_new.predict(X[test_indices])\n",
        "    \n",
        "    print('****Iteration',iter_num,'****')\n",
        "    print('Accuracy',mt.accuracy_score(Y[test_indices],y_hat))\n",
        "    print('Confusion Matrix \\n',mt.confusion_matrix(Y[test_indices],y_hat))\n",
        "    "
      ],
      "metadata": {
        "id": "BeXMjWtRxNif"
      },
      "id": "BeXMjWtRxNif",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpret Feature Importance 30\n",
        "\n",
        "Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
      ],
      "metadata": {
        "id": "5M-XYvRzh_Id"
      },
      "id": "5M-XYvRzh_Id"
    },
    {
      "cell_type": "markdown",
      "id": "a558f61e",
      "metadata": {
        "id": "a558f61e"
      },
      "source": [
        "## Looking at weights of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f74ab89",
      "metadata": {
        "id": "7f74ab89"
      },
      "outputs": [],
      "source": [
        "weights = lr_clf_new.coef_.T\n",
        "varNames = forestDF_new.columns\n",
        "for coef, name in zip(weights, varNames):\n",
        "    print(name, 'has weight of', coef[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The weights from the logistic regression model help us identify the importance of the various features. Having so many differnet features, it can be nice to see which ones impact the classification more so than others. This way, we can use those of utomst importance to create a better/more accurate model. \n",
        "\n",
        "We can see from the graph these weights for the various features quite nicely. The graph shows us that 'Elevation' has the largest absolute value. This allows us to assume that it plays the largest role in classification, in regard to all the other features in this dataset. Cache la Poudre is the next highest value. \n",
        "\n",
        "We believe that certain features have higher weights due to their immediate connection with the clasification. The Spruce/Fir trees grow best in certain conditions; some of these conditions are listed in the features. We believe that elevation is a key driver in identifying whether a tree is a Spruce/Fir. Similarly, we believe that other features directly connect to Spruce/Fir - like soil type.   "
      ],
      "metadata": {
        "id": "-EZ0E2o54Hmq"
      },
      "id": "-EZ0E2o54Hmq"
    },
    {
      "cell_type": "markdown",
      "id": "e49c6293",
      "metadata": {
        "id": "e49c6293"
      },
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f16474f",
      "metadata": {
        "id": "6f16474f"
      },
      "source": [
        "### Creating and running the SVM using the scaled and split sets from earlier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffcba57",
      "metadata": {
        "id": "2ffcba57"
      },
      "outputs": [],
      "source": [
        "# Creating Cross Validation Object - SVM\n",
        "cv_CGD = StratifiedShuffleSplit(n_splits = 3, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating SVM using linear SGD due to large amount of data\n",
        "regularize_const = 0.1\n",
        "iterations = 10\n",
        "\n",
        "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
        "                       fit_intercept = True,\n",
        "                       l1_ratio = 0.0,\n",
        "                       learning_rate = 'optimal',\n",
        "                       loss = 'hinge',\n",
        "                       max_iter = iterations,\n",
        "                       n_jobs = -1,\n",
        "                       penalty = 'l2')"
      ],
      "metadata": {
        "id": "BxrjmAXb6kfH"
      },
      "id": "BxrjmAXb6kfH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl = StandardScaler()\n",
        "\n",
        "for train_idx, test_idx in cv_CGD.split(X,Y):\n",
        "    svm_sgd.fit(scl.fit_transform(X[train_idx]), Y[train_idx])\n",
        "    y_hatSVM = svm_sgd.predict(scl.transform(X[test_idx]))\n",
        "    \n",
        "    print('Accuracy', mt.accuracy_score(Y[test_idx], y_hatSVM))\n",
        "    print('Confusion Matrix \\n', mt.confusion_matrix(Y[test_idx], y_hatSVM))"
      ],
      "metadata": {
        "id": "xPxkd8s-HT_P"
      },
      "id": "xPxkd8s-HT_P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SVMWeights = pd.Series(svm_sgd.coef_[0], index = forestDF.columns)\n",
        "SVMWeights = pd.DataFrame({'Var':SVMWeights.index, 'Weight':SVMWeights.values})\n",
        "SVMWeights = SVMWeights.sort_values('Weight')\n",
        "plt.figure(figsize = (8, 10))\n",
        "sns.barplot(x = 'Weight', y = 'Var', data = SVMWeights)"
      ],
      "metadata": {
        "id": "mkBpCba96qIl"
      },
      "id": "mkBpCba96qIl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating SVM using logistic SGD due to large amount of data\n",
        "log_sgd = SGDClassifier(alpha = regularize_const,\n",
        "                       fit_intercept = True,\n",
        "                       l1_ratio = 0.0,\n",
        "                       learning_rate = 'optimal',\n",
        "                       loss = 'log',\n",
        "                       max_iter = iterations,\n",
        "                       n_jobs = -1,\n",
        "                       penalty = 'l2')"
      ],
      "metadata": {
        "id": "UwmBAvQ0txqD"
      },
      "id": "UwmBAvQ0txqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scl = StandardScaler()\n",
        "\n",
        "for train_idx, test_idx in cv_CGD.split(X,Y):\n",
        "    log_sgd.fit(scl.fit_transform(X[train_idx]), Y[train_idx])\n",
        "    y_hatSVM = log_sgd.predict(scl.transform(X[test_idx]))\n",
        "    \n",
        "    print('Accuracy', mt.accuracy_score(Y[test_idx], y_hatSVM))\n",
        "    print('Confusion Matrix \\n', mt.confusion_matrix(Y[test_idx], y_hatSVM))"
      ],
      "metadata": {
        "id": "-sj_a7y4t30z"
      },
      "id": "-sj_a7y4t30z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logSVMWeights = pd.Series(log_sgd.coef_[0],index = forestDF.columns)\n",
        "logSVMWeights = pd.DataFrame({'Var':logSVMWeights.index, 'Weight':logSVMWeights.values})\n",
        "logSVMWeights = logSVMWeights.sort_values('Weight')\n",
        "plt.figure(figsize = (8, 10))\n",
        "sns.barplot(x = 'Weight', y = 'Var', data = logSVMWeights)"
      ],
      "metadata": {
        "id": "1VH7qxQ8tzQ-"
      },
      "id": "1VH7qxQ8tzQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub-sampling data and verifying still balanced\n",
        "forestSubDF = forestFullDF.sample(10000)\n",
        "forestSubDF.head()\n",
        "# forestSubDF['treeType'].describe().transpose()\n",
        "forestSubDF.groupby('treeType').count()"
      ],
      "metadata": {
        "id": "BuYdqQIR965A"
      },
      "id": "BuYdqQIR965A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating and Running SVM model on smaller dataframe\n",
        "if 'treeType' in forestSubDF:\n",
        "    ySub = forestSubDF['treeType'].values\n",
        "    del forestSubDF['treeType']\n",
        "    xSub = forestSubDF.values"
      ],
      "metadata": {
        "id": "rGxKwi1y94oC"
      },
      "id": "rGxKwi1y94oC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cv_iter = 10\n",
        "num_instances = len(Y)\n",
        "cv_Log = ShuffleSplit(n_splits = num_cv_iter, test_size = 0.2)"
      ],
      "metadata": {
        "id": "EOa-acTd-u28"
      },
      "id": "EOa-acTd-u28",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for train_indices, test_indices in cv_Log.split(xSub, ySub):\n",
        "    X_train = xSub[train_indices]\n",
        "    Y_train = ySub[train_indices]\n",
        "    X_test = xSub[test_indices]\n",
        "    Y_test = ySub[test_indices]\n",
        "X_train_scaled = sclObj.transform(X_train)\n",
        "X_test_scaled = sclObj.transform(X_test)"
      ],
      "metadata": {
        "id": "1RUT-VZC93Tv"
      },
      "id": "1RUT-VZC93Tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "svm_clf = SVC(C = 0.5,\n",
        "             kernel = 'rbf',\n",
        "             degree = 3,\n",
        "             gamma = 'auto')"
      ],
      "metadata": {
        "id": "1C76afxv92Cq"
      },
      "id": "1C76afxv92Cq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results\n",
        "svm_clf.fit(X_train_scaled,Y_train)\n",
        "y_hatSVM = svm_clf.predict(X_test_scaled)\n",
        "print('Accuracy', mt.accuracy_score(Y_test, y_hatSVM))\n",
        "print('Confusion Matrix \\n', mt.confusion_matrix(Y_test, y_hatSVM))"
      ],
      "metadata": {
        "id": "vhz2nI2q90kv"
      },
      "id": "vhz2nI2q90kv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector shape:\n",
        "print(svm_clf.support_vectors_.shape)\n",
        "print(svm_clf.support_.shape)\n",
        "print(svm_clf.n_support_)"
      ],
      "metadata": {
        "id": "Ro8Pk_3L9ylA"
      },
      "id": "Ro8Pk_3L9ylA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forestTested = forestSubDF.iloc[train_indices]\n",
        "forestSupport = forestTested.iloc[svm_clf.support_,:]\n",
        "forestSupport['treeType'] = ySub[svm_clf.support_]\n",
        "forestSubDF['treeType'] = ySub"
      ],
      "metadata": {
        "id": "F7Dldjd49uwK"
      },
      "id": "F7Dldjd49uwK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forestGroupSupport = forestSupport.groupby(['treeType'])\n",
        "forestGroup = forestSubDF.groupby(['treeType'])\n",
        "\n",
        "vars_to_plot = ['Elevation', 'Hillshade_Noon', 'Soil_Type22', 'Soil_Type12']\n",
        "\n",
        "for v in vars_to_plot:\n",
        "    plt.figure(figsize = (10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    ax = forestGroupSupport[v].plot.kde()\n",
        "    plt.legend(['Lodgepole Pine', 'Spruce/Fir'])\n",
        "    plt.title(v + ' (Instances chosen as Support Vectors)')\n",
        "    \n",
        "    plt.subplot(1,2,2)\n",
        "    ax = forestGroup[v].plot.kde()\n",
        "    plt.legend(['Lodgepole Pine', 'Spruce/Fir'])\n",
        "    plt.title(v + ' (Original)')"
      ],
      "metadata": {
        "id": "gQA-IoBM9rPN"
      },
      "id": "gQA-IoBM9rPN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Create Models 50\n",
        "\n",
        "Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. \n",
        "Assess how well each model performs (use 80/20 training/testing split for your data). \n",
        "Adjust parameters of the models to make them more accurate. \n",
        "If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. \n",
        "That is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. For many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
      ],
      "metadata": {
        "id": "EETbe1z0Za28"
      },
      "id": "EETbe1z0Za28"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SVM performed better than the logistic regression model. We see that the SVM gave us an accuracy score of 76%. This is a decent amount higher than the logistic score of 61%. We feel like there is still some room for improvement, but we are glad that we are seeing a higher accuracy score. "
      ],
      "metadata": {
        "id": "XG8RpRX_ZcC6"
      },
      "id": "XG8RpRX_ZcC6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpret Support Vectors(10)\n",
        "\n",
        "Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model then analyze the support vectors from the subsampled dataset."
      ],
      "metadata": {
        "id": "IeTkwGHYiUyW"
      },
      "id": "IeTkwGHYiUyW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at some of the support vectors that were chosen, we can see that they greatly affect the outcome of the calssification. The graphs above do a great job illustrating the change in accuracy when comparing the original and the Support Vector.  "
      ],
      "metadata": {
        "id": "hYUF-om66b6s"
      },
      "id": "hYUF-om66b6s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Advantages 10\n",
        "Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail."
      ],
      "metadata": {
        "id": "ZDkhl8Y-h3f_"
      },
      "id": "ZDkhl8Y-h3f_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The interpretability of Logistic Regression was fairly easy and understandable. This model also allowed for an easier way to see various accuracy scores for different train/test splits. \n",
        "\n",
        "The SVM model provided a better accuracy score, which shows its superiority to Logistic Regression in this particular case. And using certain plots, it is easier to visually see the outcome of the model. The SVM seemed to be slightly quicker than Logistic Regression as well. We can see that both models are decently quick to run, but the SVM is faster. "
      ],
      "metadata": {
        "id": "Yxt3r6dQa3NC"
      },
      "id": "Yxt3r6dQa3NC"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}